{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mangement and Mining - Group Assessment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Instructions: Analysing, Processing, and Modeling Financial Transaction Data\n",
    "\n",
    "Your team has been given access to a dataset containing financial transaction data. Your goal is to analyze, process, and model this data to identify patterns, trends, and potential fraud.Once completed you will need to submit this notebook and an accompanying model file for assessment (See Assessment Brief Section 2 Part 1). Once an initial submission has been made groups should focus on improving the model and the analysis, then resubmit the updated notebook and model file (See Assessment Brief section 2 Part 2). The final submission should also include a short reflective summary, outlining the changes made and the reasons for these changes (See assessment brief Section 2 Part 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Dataset\n",
    "\n",
    "The dataset provided for the assessment contains a sample of financial transactions, made by customers. The features of the dataset are described below, they mainly include information about the transaction and the customer involved in the transaction. The dataset also contains a binary target variable called 'Is.Fraudulent', which indicates whether the transaction is fraudulent or not. The goal of the assessment is to build a model that can predict whether a transaction is fraudulent or not based on the features provided in the dataset and any additional features that you may create. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable          | Data Type     | Description             |\n",
    "|-------------------|---------------|-------------------------|\n",
    "| Transaction.Date   | object        | The date of the transaction. |\n",
    "| Transaction.Amount | float64       | The amount of money involved. |\n",
    "| Customer.Age      | int64         | The age of the customer.    |\n",
    "| Account.Age.Days  | int64         | The number of days since opened. |\n",
    "| Transaction.Hour  | int64         | The hour of day during transaction. |\n",
    "| source            | object        | The source of the transaction.|\n",
    "| browser           | object        | The browser used for transaction.|\n",
    "| Payment.Method   | object        | The payment method used.    |\n",
    "| Product.Category | object        | The category of product purchased.|\n",
    "| Quantity          | int64         | The number of units purchased.|\n",
    "| Device.Used       | object        | The device used to make transaction.|\n",
    "| Is.Fraudulent     | int64         | A flag indicating fraudulent status.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Template \n",
    "\n",
    "The template provided below is a guide to help you structure your analysis. You can add additional code and text cells to the template as needed. There are cells and code throughout the template which you should not modify, these are required to ensure the assessment can be marked successfully. The cells or code you should not modify will be clearly labeled. Please ensure you save a copy of the template to your local machine before you start your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240000 entries, 0 to 239999\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Transaction.Date    240000 non-null  object \n",
      " 1   Transaction.Amount  240000 non-null  float64\n",
      " 2   Customer.Age        240000 non-null  int64  \n",
      " 3   Account.Age.Days    240000 non-null  int64  \n",
      " 4   Transaction.Hour    240000 non-null  int64  \n",
      " 5   source              240000 non-null  object \n",
      " 6   browser             240000 non-null  object \n",
      " 7   Payment.Method      240000 non-null  object \n",
      " 8   Product.Category    240000 non-null  object \n",
      " 9   Quantity            240000 non-null  int64  \n",
      " 10  Device.Used         240000 non-null  object \n",
      " 11  Is.Fraudulent       240000 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(6)\n",
      "memory usage: 22.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# load the data\n",
    "df = pd.read_csv('student_dataset.csv')\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "# df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Data Shape: (240000, 24)\n",
      "\n",
      "Processed Data Columns: ['Transaction.Amount', 'Customer.Age', 'Account.Age.Days', 'Transaction.Hour', 'Quantity', 'Transaction_Day', 'Transaction_Month', 'Transaction_Year', 'source_Direct', 'source_SEO', 'browser_FireFox', 'browser_IE', 'browser_Opera', 'browser_Safari', 'Payment.Method_bank transfer', 'Payment.Method_credit card', 'Payment.Method_debit card', 'Product.Category_electronics', 'Product.Category_health & beauty', 'Product.Category_home & garden', 'Product.Category_toys & games', 'Device.Used_mobile', 'Device.Used_tablet', 'Is.Fraudulent']\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     44544\n",
      "           1       0.96      0.40      0.57      3456\n",
      "\n",
      "    accuracy                           0.96     48000\n",
      "   macro avg       0.96      0.70      0.77     48000\n",
      "weighted avg       0.96      0.96      0.95     48000\n",
      "\n",
      "F1 Score: 0.565\n"
     ]
    }
   ],
   "source": [
    "# continue to analyze the data\n",
    "\n",
    "# Data Preprocessing Function\n",
    "def preprocess_data(df_raw):\n",
    "    \"\"\"Preprocess the financial transaction data\"\"\"\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Convert Transaction.Date to datetime with mixed format handling\n",
    "    df['Transaction.Date'] = pd.to_datetime(df['Transaction.Date'], format='mixed', errors='coerce')\n",
    "    df['Transaction_Day'] = df['Transaction.Date'].dt.day\n",
    "    df['Transaction_Month'] = df['Transaction.Date'].dt.month\n",
    "    df['Transaction_Year'] = df['Transaction.Date'].dt.year\n",
    "    \n",
    "    # Handle missing values in date-derived columns\n",
    "    df['Transaction_Day'] = df['Transaction_Day'].fillna(df['Transaction_Day'].median())\n",
    "    df['Transaction_Month'] = df['Transaction_Month'].fillna(df['Transaction_Month'].median())\n",
    "    df['Transaction_Year'] = df['Transaction_Year'].fillna(df['Transaction_Year'].median())\n",
    "    \n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_columns = ['Transaction.Amount', 'Customer.Age', 'Account.Age.Days', \n",
    "                      'Transaction.Hour', 'Quantity']\n",
    "    for col in numeric_columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_columns = ['source', 'browser', 'Payment.Method', \n",
    "                         'Product.Category', 'Device.Used']\n",
    "    df_encoded = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "    \n",
    "    # Combine features\n",
    "    numeric_features = df[numeric_columns + ['Transaction_Day', 'Transaction_Month', 'Transaction_Year']]\n",
    "    df_processed = pd.concat([numeric_features, df_encoded], axis=1)\n",
    "    df_processed['Is.Fraudulent'] = df['Is.Fraudulent']\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed = preprocess_data(df)\n",
    "print(\"\\nProcessed Data Shape:\", df_processed.shape)\n",
    "print(\"\\nProcessed Data Columns:\", df_processed.columns.tolist())\n",
    "\n",
    "# Split features and target\n",
    "X = df_processed.drop('Is.Fraudulent', axis=1)\n",
    "y = df_processed['Is.Fraudulent']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed your analysis and are ready to submit the assessment you should export the trained model file (**only one model will be accepted**). The model file should be saved as a pickle file (.pkl). The model file should be saved in the same directory as the notebook. Once you have saved the model file you should upload both the notebook and the model file to the assessment submission portal. Please ensure you provide the model file name as a variable, see example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not delete this cell ##\n",
    "\n",
    "# export the model with pickle\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "\n",
    "# define the filename, it should have a .pkl extension\n",
    "filename = 'log_reg_model.pkl' # replace 'log_reg_model' with the name of your model variable\n",
    "\n",
    "# save the model to the current directory\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(rf_model, f) # replace 'log_reg_model' with the name of your model variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment Evaluation\n",
    "\n",
    "This is required for the assessment to be marked. Groups should specify any data processing steps that are required to run the model in the cell below. This may include the installation of additional libraries, loading of the data, and any additional processing steps required to run the model. The model should be saved to a file called 'model.pkl' in the same directory as the notebook. The model file should be loaded and tested in the cell below to ensure it runs correctly. The model should be loaded and tested using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not delete this cell ##\n",
    "\n",
    "# load the evaluation data\n",
    "import pandas as pd\n",
    "\n",
    "# load the raw data\n",
    "df_eval_raw = pd.read_csv('evaluation_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups should add the necessary preprocessing steps to prepare the data for evaluation below \n",
    "\n",
    "\n",
    "\n",
    "# the final dataset should be saved in a DataFrame called df_eval\n",
    "df_eval = preprocess_data(df_eval_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Do not delete this cell ##\n",
    "\n",
    "# Load the model and evaluate it on the evaluation data \n",
    "\n",
    "# load the pickle model\n",
    "with open(filename, \"rb\") as f:\n",
    "    eval_model = pickle.load(f) # do not change the name of the model variable\n",
    "\n",
    "# test the model on the evaluation data\n",
    "y_eval = eval_model.predict(df_eval.drop('Is.Fraudulent', axis=1))\n",
    "\n",
    "# calculate the f1 score\n",
    "f1_eval = f1_score(df_eval['Is.Fraudulent'], y_eval)\n",
    "\n",
    "# print the f1 score\n",
    "print(f'F1 Score: {f1_eval:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
